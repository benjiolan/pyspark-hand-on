{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f58fcb7",
   "metadata": {},
   "source": [
    "## 86. Funciones definidas por el usuario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d3abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 23:43:13 WARN Utils: Your hostname, personal-computer resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "25/11/04 23:43:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/04 23:43:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/04 23:43:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "path = os.environ.get(\"HOME\")\n",
    "os.chdir(f\"{path}/github/pyspark-hand-on\")\n",
    "from src.utils.utils import spark_session\n",
    "\n",
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d474fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubo(n):\n",
    "    return n**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea8cf9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "146f4d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 00:03:46 WARN SimpleFunctionRegistry: The function cubo replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.cubo(n)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Registrar función como udf:\n",
    "spark.udf.register(\"cubo\", cubo, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d7c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1, 10).createOrReplaceTempView(\"df_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edf80137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|cubo|\n",
      "+---+----+\n",
      "|  1|   1|\n",
      "|  2|   8|\n",
      "|  3|  27|\n",
      "|  4|  64|\n",
      "|  5| 125|\n",
      "|  6| 216|\n",
      "|  7| 343|\n",
      "|  8| 512|\n",
      "|  9| 729|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubo(id) AS cubo FROM df_temp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01df573",
   "metadata": {},
   "source": [
    "#### 2° forma de usar udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cee5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bienvenida(nombre):\n",
    "    return \"Hola {}\".format(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5743c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bienvenida_udf = udf(lambda x: bienvenida(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fee56385",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creacion de dataframe:\n",
    "df_name = spark.createDataFrame([(\"Jose\",), (\"Julia\",)], [\"nombre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94c9da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- pago: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d7db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|Jose  |1   |\n",
      "|Julia |2   |\n",
      "|Katia |1   |\n",
      "|NULL  |3   |\n",
      "|Raul  |3   |\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51b7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfd978af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|nombre|bienvenida_nombre|\n",
      "+------+-----------------+\n",
      "|  Jose|        Hola Jose|\n",
      "| Julia|       Hola Julia|\n",
      "| Katia|       Hola Katia|\n",
      "|  NULL|        Hola None|\n",
      "|  Raul|        Hola Raul|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col(\"nombre\"),\n",
    "    bienvenida_udf(col(\"nombre\")).alias(\"bienvenida_nombre\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a0c2a",
   "metadata": {},
   "source": [
    "#### 3° Forma de crear un udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdda5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def mayuscula(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "101325a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|nombre|nombre|\n",
      "+------+------+\n",
      "|  Jose|  JOSE|\n",
      "| Julia| JULIA|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_name.select(\n",
    "    col(\"nombre\"),\n",
    "    mayuscula(col(\"nombre\")).alias(\"nombre\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148f102",
   "metadata": {},
   "source": [
    "#### UDF de Pandas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824dc91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "470aca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41fffcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubo_pandas(series:pd.Series)->pd.Series:\n",
    "    return series**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e8c89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubo_udf_pd = pandas_udf(cubo_pandas, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24870c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fec5d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cubo_pandas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77ceef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cbd62a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|cubo_pandas|\n",
      "+---+-----------+\n",
      "|  0|          0|\n",
      "|  1|          1|\n",
      "|  2|          8|\n",
      "|  3|         27|\n",
      "|  4|         64|\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    col(\"id\"),\n",
    "    cubo_udf_pd(col(\"id\")).alias(\"cubo_pandas\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f493fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-hand-on (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
